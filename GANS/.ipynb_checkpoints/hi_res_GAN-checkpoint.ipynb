{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a3b894-4b04-4b2a-82a4-e093b3867b71",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99bcae-459e-4755-8e88-24cbbacb1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from enum import Enum\n",
    "import math\n",
    "from typing import List, Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16874f61-c983-46ab-9553-d4fae8051c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import conv_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5ffa8-3832-4690-b99c-a0d98330d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from create_datetime_str import create_datetime_str\n",
    "from default_to import default_to\n",
    "from limit_gpu_memory_usage import limit_gpu_memory_usage\n",
    "from checkpointer import Checkpointer\n",
    "import math\n",
    "import os\n",
    "from perceptual_difference import create_perceptual_difference_model\n",
    "import tensorflow as tf\n",
    "from tensor_ops import pixel_norm, lerp\n",
    "from train import TrainingState\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61dd46-2ed8-4596-8f3b-38111cb85fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from default_to import default_to\n",
    "from generator_visualizer_callback import GeneratorVisualizer\n",
    "from tf_image_records import decode_record_image\n",
    "from typing import List, Optional, Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba543bd-87bd-4acf-be95-5e09855f642a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad1893-3abf-4df5-a20b-50dcb9f6e30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3962113-81e0-47d9-af41-afa39f18a4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d57cd50-6b98-45cd-8fe0-8ff0f3f60109",
   "metadata": {},
   "source": [
    "## File IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec0928-0ad9-4e57-90b1-42e3896fdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(file_path: os.PathLike, content: str, open_mode: str = 'w') -> None:\n",
    "    ensure_dir_for_file(file_path)\n",
    "    with open(file_path, open_mode) as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def write_binary_file(file_path: os.PathLike, content: bytes, open_mode: str = 'wb') -> None:\n",
    "    ensure_dir_for_file(file_path)\n",
    "    with open(file_path, open_mode) as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def write_lines_to_file(file_path: os.PathLike, lines: Iterable[str]):\n",
    "    ensure_dir_for_file(file_path)\n",
    "    write_text_file(file_path, '\\n'.join(lines))\n",
    "\n",
    "\n",
    "def read_text_file(file_path: os.PathLike, open_mode: str = 'r') -> str:\n",
    "    with open(file_path, open_mode) as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def read_binary_file(file_path: os.PathLike, open_mode: str = 'rb') -> bytes:\n",
    "    with open(file_path, open_mode) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe87dd-5374-4f11-969f-6399ac168df6",
   "metadata": {},
   "source": [
    "## Basic Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2e8c3-bc37-4a2d-8820-0c8632cd40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_tpu_strategy(tpu_name=''):\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu_name)\n",
    "        return tf.distribute.TPUStrategy(tpu)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_strategy(tpu_name=''):\n",
    "    return default_to_fn(\n",
    "        try_create_tpu_strategy(tpu_name=tpu_name),\n",
    "        tf.distribute.MirroredStrategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392bc70-6c97-4ead-8c8b-1a177c398d51",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e392293-6eb9-4ea6-b311-6f65175daff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime_str() -> str:\n",
    "    return f'{datetime.datetime.now():%Y-%m-%d_%H_%M_%S_%f}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13dc3e-20ab-4cb1-a208-06bf08fa3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(dir_name):\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "def ensure_dir_for_file(file_path):\n",
    "    ensure_dir(os.path.dirname(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267d657-8bf1-4845-aaf3-57d953d7bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T')\n",
    "\n",
    "\n",
    "def default_to_fn(optional_value: Optional[T], make_default: Callable[[], T]) -> T:\n",
    "    return make_default() if optional_value is None else optional_value\n",
    "\n",
    "\n",
    "def default_to(optional_value: Optional[T], default: T) -> T:\n",
    "    return default_to_fn(optional_value, lambda: default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef66f0-8deb-4be0-86c5-c53dc55a48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(path: os.PathLike, image: tf.Tensor) -> None:\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    if image.dtype != tf.uint8:\n",
    "        image = tf.image.convert_image_dtype(image, tf.uint8, saturate=True)\n",
    "    bk_io.write_binary_file(path, tf.io.encode_png(image).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2eebbd-772f-479f-89be-acaea67d3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_record_image(image: tf.Tensor):\n",
    "    image_shape = image.shape.as_list()\n",
    "    assert image.dtype == tf.uint8\n",
    "    assert len(image_shape) == 3\n",
    "    for x in image_shape:\n",
    "        assert x is not None\n",
    "        assert type(x) == int\n",
    "    image_bytes = tf.io.encode_png(image).numpy()\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'image_shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image_shape)),\n",
    "                'image_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "                })\n",
    "        ).SerializeToString()\n",
    "\n",
    "\n",
    "def decode_record_image(record_bytes):\n",
    "    schema = {\n",
    "        'image_shape': tf.io.FixedLenFeature([3], dtype=tf.int64),\n",
    "        'image_bytes': tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "        }\n",
    "    example = tf.io.parse_single_example(record_bytes, schema)\n",
    "    image = tf.io.decode_image(example['image_bytes'])\n",
    "    image = tf.reshape(image, tf.cast(example['image_shape'], tf.int32))\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453da0e-12dd-403a-90db-130e3332cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(unserialized: object) -> bytes:\n",
    "    return pickle.dumps(unserialized)\n",
    "\n",
    "\n",
    "def deserialize(serialized: bytes):\n",
    "    return pickle.loads(serialized)\n",
    "\n",
    "\n",
    "'''\n",
    "    I'm using np.save() over tf.io.serialize_tensor because parse_tensor\n",
    "    (https://www.tensorflow.org/api_docs/python/tf/io/parse_tensor) requires the caller to know\n",
    "    the correct dtype whereas np.save() does not.\n",
    "\n",
    "    Adapted from:\n",
    "    https://stackoverflow.com/questions/30698004/how-can-i-serialize-a-numpy-array-while-preserving-matrix-dimensions\n",
    "'''\n",
    "def serialize_array(arr: np.ndarray) -> bytes:\n",
    "    mem_file = io.BytesIO()\n",
    "\n",
    "    np.save(mem_file, arr)\n",
    "\n",
    "    mem_file.seek(0)\n",
    "    return mem_file.read()\n",
    "\n",
    "\n",
    "def deserialize_array(serialized: bytes) -> np.ndarray:\n",
    "    mem_file = io.BytesIO()\n",
    "\n",
    "    mem_file.write(serialized)\n",
    "\n",
    "    mem_file.seek(0)\n",
    "    return np.load(mem_file)\n",
    "\n",
    "\n",
    "def serialize_tensor(t: tf.Tensor) -> bytes:\n",
    "    return serialize_array(t.numpy())\n",
    "\n",
    "\n",
    "def serialize_variable(v: tf.Variable) -> bytes:\n",
    "    return serialize_tensor(v)\n",
    "\n",
    "\n",
    "def deserialize_tensor(serialized: bytes) -> tf.Tensor:\n",
    "    return tf.constant(deserialize_array(serialized))\n",
    "\n",
    "\n",
    "def deserialize_variable(serialized: bytes) -> tf.Variable:\n",
    "    return tf.Variable(deserialize_array(serialized))\n",
    "\n",
    "\n",
    "'''\n",
    "    Annoyingly, keras does not provide a way to serialize a model directly to a bytes object. The\n",
    "    save()/load_model() API requires a local filesystem, which is commonly not available cloud TPUs.\n",
    "    The to_json() API does not store weights and stores intermediate dtype information so that a\n",
    "    model trained on bfloat16 cannot be loaded on a machine that doesn't support bfloat16. For now,\n",
    "    we'll just stored the weights and assume the deserializer knows how to create the model\n",
    "    architecture.\n",
    "'''\n",
    "def serialize_model(model: tf.keras.Model) -> bytes:\n",
    "    return serialize({\n",
    "        'model_weights': list(map(serialize_array, model.get_weights()))\n",
    "    })\n",
    "\n",
    "\n",
    "def deserialize_model(serialized: bytes, create_model: Callable[[], tf.keras.Model]) -> tf.keras.Model:\n",
    "    serializable = deserialize(serialized)\n",
    "    model = create_model()\n",
    "    model.set_weights(list(map(deserialize_array, serializable['model_weights'])))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512640f2-33e4-4fee-8aeb-b118928270cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c335e-5d6e-4059-8cca-d5ccaef56fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09824ebd-caf2-4fd2-aabd-f606fcb90bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7ac55-09c1-4cb7-9004-36032de7c848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdde5310-25ec-4769-9963-34baf1f1c704",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7f4c1-f715-44ce-87ae-cba961d5e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorVisualizer(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            strategy: tf.distribute.Strategy,\n",
    "            grid_size: Tuple[int, int],\n",
    "            generator: tf.keras.Model,\n",
    "            noise: tf.Tensor,\n",
    "            on_image_callbacks: List[Callable[[int, tf.Tensor], None]] = list(),\n",
    "            update_interval: int = 1,\n",
    "            replica_batch_size: int = 8\n",
    "            ):\n",
    "        self.generator = generator\n",
    "        self.grid_size = grid_size\n",
    "        self.image_count = grid_size[0] * grid_size[1]\n",
    "        self.strategy = strategy\n",
    "        self.replica_batch_size = replica_batch_size\n",
    "        self.noise = noise\n",
    "        assert self.noise.shape[0] == grid_size[0] * grid_size[1]\n",
    "        self.update_interval = update_interval\n",
    "        self.on_image_callbacks = on_image_callbacks\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, samples):\n",
    "        return self.generator(samples, training=False)\n",
    "\n",
    "    def predict_in_batches(self) -> tf.Tensor:\n",
    "        global_batch_size = self.replica_batch_size * self.strategy.num_replicas_in_sync\n",
    "        sample_count = self.noise.shape[0]\n",
    "        batch_count = math.ceil(sample_count / global_batch_size)\n",
    "        padded_sample_count = batch_count * global_batch_size\n",
    "        padded_input = tf.concat(\n",
    "            [self.noise,\n",
    "            tf.zeros((padded_sample_count - sample_count,) + self.noise.shape[1:], dtype=self.noise.dtype)],\n",
    "            0)\n",
    "\n",
    "        dataset = self.strategy.experimental_distribute_dataset(\n",
    "            tf.data.Dataset.from_tensor_slices(padded_input).batch(global_batch_size))\n",
    "\n",
    "        batch_outputs = []\n",
    "\n",
    "        for batch_samples in dataset:\n",
    "            batch_outputs.append(\n",
    "                self.strategy.gather(\n",
    "                    self.strategy.run(self.predict, args=(batch_samples,)),\n",
    "                    0))\n",
    "\n",
    "        padded_output = tf.concat(batch_outputs, 0)\n",
    "        return padded_output[:sample_count]\n",
    "\n",
    "    def generate_image(self) -> tf.Tensor:\n",
    "        # Note: calling self.generator.predict(self.noise, batch_size=self.batch_size)\n",
    "        # caused the next iterations of the training loop to generate NaNs with a\n",
    "        # high probability, especially on larger resolutions. I didn't find an\n",
    "        # obvious reason for that. And since it's incredibly tangential to the\n",
    "        # course topic, I went with a simple work-around.\n",
    "        images = self.predict_in_batches()\n",
    "        images = tf.image.convert_image_dtype(images, tf.uint8, saturate=True)\n",
    "\n",
    "        return tf.reshape(\n",
    "            images,\n",
    "            tf.concat(\n",
    "                [self.grid_size, tf.shape(images)[1:]],\n",
    "                0))\n",
    "\n",
    "    def on_epoch_end(self, epoch_i: int, logs=None) -> None:\n",
    "        if epoch_i % self.update_interval != 0:\n",
    "            return\n",
    "\n",
    "        image = self.generate_image()\n",
    "\n",
    "        for callback in self.on_image_callbacks:\n",
    "          callback(epoch_i, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de5234-6d6d-4ad5-addb-cbe2d10b3482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b6323-1527-420c-ad34-6529c0563064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663c2a6-fdd6-4975-b397-e718db980a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7beda9-87ed-48a9-a605-89b891dd0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blur_filter(dtype):\n",
    "    return tf.constant(\n",
    "        [[0.015625, 0.046875, 0.046875, 0.015625],\n",
    "         [0.046875, 0.140625, 0.140625, 0.046875],\n",
    "         [0.046875, 0.140625, 0.140625, 0.046875],\n",
    "         [0.015625, 0.046875, 0.046875, 0.015625]],\n",
    "        dtype=dtype)\n",
    "\n",
    "\n",
    "def blur(x, strides=1):\n",
    "    channel_count = x.shape[3]\n",
    "    filter = create_blur_filter(x.dtype)[:, :, tf.newaxis, tf.newaxis]\n",
    "    filter = tf.tile(filter, [1, 1, channel_count, 1])\n",
    "\n",
    "    return tf.nn.depthwise_conv2d(x, filter, strides=[1, strides, strides, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def upsample(x):\n",
    "    def upsample_with_zeros(x):\n",
    "        in_height = x.shape[1]\n",
    "        in_width = x.shape[2]\n",
    "        channel_count = x.shape[3]\n",
    "\n",
    "        out_height = in_height * 2\n",
    "        out_width = in_width * 2\n",
    "        x = tf.reshape(x, [-1, in_height, 1, in_width, 1, channel_count])\n",
    "        x = tf.pad(x, [[0, 0], [0, 0], [0, 1], [0, 0], [0, 1], [0, 0]])\n",
    "        return tf.reshape(x, [-1, out_height, out_width, channel_count])\n",
    "\n",
    "    return blur(upsample_with_zeros(x*4.))\n",
    "\n",
    "\n",
    "def downsample(x):\n",
    "    return blur(x, strides=2)\n",
    "\n",
    "\n",
    "def pixel_norm(x: tf.Tensor, epsilon: float = 1e-7) -> tf.Tensor:\n",
    "    original_dtype = x.dtype\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    normalized = x / tf.math.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + epsilon)\n",
    "    return tf.cast(normalized, original_dtype)\n",
    "\n",
    "\n",
    "def lerp(start: tf.Tensor, end: tf.Tensor, factor: tf.Tensor) -> tf.Tensor:\n",
    "    return start + (end - start) * factor\n",
    "\n",
    "\n",
    "def reduce_std_nan_safe(x, axis=None, keepdims=False, epsilon=1e-7):\n",
    "    y = tf.cast(x, tf.float32)\n",
    "    mean = tf.reduce_mean(y, axis=axis, keepdims=True)\n",
    "    variance = tf.reduce_mean(tf.square(y - mean), axis=axis, keepdims=keepdims)\n",
    "    sqrt = tf.sqrt(variance + epsilon)\n",
    "    return tf.cast(sqrt, x.dtype)\n",
    "\n",
    "\n",
    "def minibatch_stddev(x: tf.Tensor, group_size=4) -> tf.Tensor:\n",
    "    original_shape = tf.shape(x)\n",
    "    original_dtype = x.dtype\n",
    "    global_sample_count = original_shape[0]\n",
    "    group_size = tf.minimum(group_size, global_sample_count)\n",
    "    group_count = global_sample_count // group_size\n",
    "    tf.Assert(\n",
    "        group_size * group_count == global_sample_count,\n",
    "        ['Sample count was not divisible by group size'])\n",
    "    # Shape definitions:\n",
    "    # N = global sample count\n",
    "    # G = group count\n",
    "    # M = sample count within group\n",
    "    # H = height\n",
    "    # W = width\n",
    "    # C = channel count\n",
    "                                                                # [NHWC] Input shape\n",
    "    y = tf.reshape(\n",
    "        x,\n",
    "        tf.concat([[-1, group_size], original_shape[1:]], 0))   # [GMHWC] Split into groups\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    stddevs = reduce_std_nan_safe(y, axis=1, keepdims=True)     # [G1HWC]\n",
    "    avg = tf.reduce_mean(\n",
    "        stddevs,\n",
    "        axis=tf.range(1, tf.rank(stddevs)),\n",
    "        keepdims=True)                                          # [G1111]\n",
    "    new_feature_shape = tf.concat([tf.shape(y)[:-1], [1]], 0)   # [GMHW1]\n",
    "    new_feature = tf.broadcast_to(avg, new_feature_shape)\n",
    "    y = tf.concat([y, new_feature], axis=-1)\n",
    "    y = tf.reshape(\n",
    "\t\ty,\n",
    "\t\ttf.concat([[-1], tf.shape(y)[2:]], 0))\n",
    "    y = tf.cast(y, original_dtype)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dd027-80c9-4462-8f01-a12532c25975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(tf.keras.layers.Layer):\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        return pixel_norm(x)\n",
    "\n",
    "\n",
    "class Upsample(tf.keras.layers.Layer):\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        return upsample(x)\n",
    "\n",
    "\n",
    "class Downsample(tf.keras.layers.Layer):\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        return downsample(x)\n",
    "\n",
    "\n",
    "class MinibatchStddev(tf.keras.layers.Layer):\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        return minibatch_stddev(x)\n",
    "\n",
    "\n",
    "class ScaledLeakyRelu(tf.keras.layers.Layer):\n",
    "    def __init__(self, alpha: float = 0.2, gain: float = math.sqrt(2.), **kwargs):\n",
    "        super(ScaledLeakyRelu, self).__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gain = gain\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        return tf.nn.leaky_relu(x, self.alpha) * self.gain\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ScaledLeakyRelu, self).get_config()\n",
    "        config.update({\n",
    "            'alpha': self.alpha,\n",
    "            'gain': self.gain\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class ImageConversionMode(Enum):\n",
    "    TENSORFLOW_TO_MODEL = 0  # [0, 1] to [-1, 1]\n",
    "    MODEL_TO_TENSORFLOW = 1  # [-1, 1] to [0, 1]\n",
    "\n",
    "\n",
    "class ImageConversion(tf.keras.layers.Layer):\n",
    "    def __init__(self, conversion_mode: ImageConversionMode, **kwargs):\n",
    "        super(ImageConversion, self).__init__(**kwargs)\n",
    "        self.conversion_mode = ImageConversionMode(conversion_mode)\n",
    "\n",
    "    def call(self, image: tf.Tensor) -> tf.Tensor:\n",
    "        if self.conversion_mode == ImageConversionMode.TENSORFLOW_TO_MODEL:\n",
    "            return image * 2. - 1.  # [0, 1] to [-1, 1]\n",
    "        elif self.conversion_mode == ImageConversionMode.MODEL_TO_TENSORFLOW:\n",
    "            return image * 0.5 + 0.5  # [-1, 1] to [0, 1]\n",
    "        else:\n",
    "            assert False, f'Unknown conversion mode: {self.conversion_mode}'\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ImageConversion, self).get_config()\n",
    "        config.update({\n",
    "            'conversion_mode': self.conversion_mode.value\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class ScaledAdd(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale: float = 1. / math.sqrt(2.), **kwargs):\n",
    "        super(ScaledAdd, self).__init__(**kwargs)\n",
    "        self.scale_value = scale\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        assert len(input_shapes) == 2\n",
    "        a_shape, b_shape = input_shapes\n",
    "        assert a_shape[1:] == b_shape[1:], f'{a_shape} != {b_shape}'\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            initializer=tf.keras.initializers.Constant(value=self.scale_value),\n",
    "            trainable=False)\n",
    "\n",
    "    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:\n",
    "        return (inputs[0] + inputs[1]) * self.scale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ScaledAdd, self).get_config()\n",
    "        config.update({\n",
    "            'scale': self.scale_value\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class ScaledConv2d(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channel_count: int,\n",
    "            kernel_size: int,\n",
    "            strides: int = 1,\n",
    "            padding: str = 'valid',\n",
    "            pre_blur: bool = False,\n",
    "            **kwargs):\n",
    "        super(ScaledConv2d, self).__init__(**kwargs)\n",
    "        self.rank = 2\n",
    "        self.channel_count = channel_count\n",
    "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank, 'kernel_size')\n",
    "        self.strides = conv_utils.normalize_tuple(strides, self.rank, 'strides')\n",
    "        self.padding = conv_utils.normalize_padding(padding)\n",
    "        self.pre_blur = pre_blur\n",
    "\n",
    "    def build(self, input_shape: List[int]) -> None:\n",
    "        assert len(input_shape) == self.rank + 2\n",
    "        in_channel_count = input_shape[-1]\n",
    "        kernel_shape = self.kernel_size + (in_channel_count, self.channel_count)\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=tf.keras.initializers.random_normal(mean=0., stddev=1.),\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.channel_count,),\n",
    "            initializer=tf.keras.initializers.zeros(),\n",
    "            trainable=True)\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            shape=(),\n",
    "            initializer=tf.keras.initializers.constant(\n",
    "                1. / tf.sqrt(tf.reduce_prod(tf.cast(kernel_shape[:-1], tf.float32)))),\n",
    "            trainable=False)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        y = x\n",
    "        if self.pre_blur:\n",
    "            y = blur(y)\n",
    "        y = tf.nn.conv2d(y, self.kernel * self.scale, strides=self.strides, padding=self.padding.upper())\n",
    "        y = tf.nn.bias_add(y, self.bias)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ScaledConv2d, self).get_config()\n",
    "        config.update({\n",
    "            'channel_count': self.channel_count,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'pre_blur': self.pre_blur\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "'''Replaces an upscale2d + conv2d sequence\n",
    "\n",
    "Based on progan/stylegan code, this is \"Faster and uses less memory than performing the operations\n",
    "separately.\"\n",
    "\n",
    "This is designed to upsample by 2x using a 3x3 convolution and does not accept any parameters other\n",
    "than the channel count\n",
    "'''\n",
    "class UpsampleConv2d(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channel_count: int,\n",
    "            **kwargs):\n",
    "        super(UpsampleConv2d, self).__init__(**kwargs)\n",
    "        self.rank = 2\n",
    "        self.channel_count = channel_count\n",
    "        self.kernel_size = conv_utils.normalize_tuple(3, self.rank, 'kernel_size')\n",
    "        self.strides = conv_utils.normalize_tuple(2, self.rank, 'strides')\n",
    "        self.padding = conv_utils.normalize_padding('same')\n",
    "\n",
    "    def build(self, input_shape: List[int]) -> None:\n",
    "        assert len(input_shape) == self.rank + 2\n",
    "        in_channel_count = input_shape[-1]\n",
    "        kernel_shape = self.kernel_size + (self.channel_count, in_channel_count)\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer=tf.keras.initializers.random_normal(mean=0., stddev=1.),\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.channel_count,),\n",
    "            initializer=tf.keras.initializers.zeros(),\n",
    "            trainable=True)\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            shape=(),\n",
    "            initializer=tf.keras.initializers.constant(\n",
    "                1. / tf.sqrt(tf.cast(tf.reduce_prod(kernel_shape) // self.channel_count, tf.float32))),\n",
    "            trainable=False)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = tf.TensorShape(input_shape).as_list()\n",
    "        return tf.TensorShape([\n",
    "            input_shape[0],\n",
    "            input_shape[1] * 2,\n",
    "            input_shape[2] * 2,\n",
    "            self.channel_count])\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size, in_height, in_width = input_shape[0], input_shape[1], input_shape[2]\n",
    "        output_shape = (batch_size, in_height * 2, in_width * 2, self.channel_count)\n",
    "\n",
    "        y = tf.nn.conv2d_transpose(\n",
    "            x,\n",
    "            self.kernel * self.scale,\n",
    "            output_shape,\n",
    "            self.strides,\n",
    "            padding=self.padding.upper())\n",
    "\n",
    "        if not tf.executing_eagerly():\n",
    "            y.set_shape(self.compute_output_shape(x.shape))\n",
    "\n",
    "        y = tf.nn.bias_add(y, self.bias)\n",
    "        y = blur(y)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(UpsampleConv2d, self).get_config()\n",
    "        config.update({\n",
    "            'channel_count': self.channel_count\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class ScaledDense(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_count: int,\n",
    "            **kwargs):\n",
    "        super(ScaledDense, self).__init__(**kwargs)\n",
    "        self.output_count = output_count\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=[input_shape[-1], self.output_count],\n",
    "            initializer=tf.keras.initializers.random_normal(mean=0., stddev=1.),\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.output_count,),\n",
    "            initializer=tf.keras.initializers.zeros(),\n",
    "            trainable=True)\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            shape=(),\n",
    "            initializer=tf.keras.initializers.constant(1. / math.sqrt(input_shape[-1])),\n",
    "            trainable=False)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        y = tf.matmul(x, self.kernel * self.scale)\n",
    "        return tf.nn.bias_add(y, self.bias)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ScaledDense, self).get_config()\n",
    "        config.update({\n",
    "            'output_count': self.output_count\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec901a3-275d-4a88-a169-92787198740f",
   "metadata": {},
   "source": [
    "### Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc76f5-0286-4c19-8336-5224758f1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_resolution(resolution: int) -> None:\n",
    "    assert resolution in [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "\n",
    "def activate(\n",
    "        x,\n",
    "        activation = lambda: ScaledLeakyRelu()):\n",
    "    if activation is not None:\n",
    "        x = activation()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_2d(\n",
    "        x,\n",
    "        channel_count: int,\n",
    "        kernel_size: int = 3,\n",
    "        activation = lambda: ScaledLeakyRelu(),\n",
    "        padding: str = 'same',\n",
    "        strides: int = 1,\n",
    "        pre_blur: bool = False,\n",
    "        name: Optional[str] = None):\n",
    "    x = ScaledConv2d(\n",
    "        channel_count,\n",
    "        kernel_size,\n",
    "        padding=padding,\n",
    "        strides=strides,\n",
    "        pre_blur=pre_blur,\n",
    "        name=name)(x)\n",
    "    return activate(x, activation=activation)\n",
    "\n",
    "\n",
    "def create_generator_body(resolution, latent_vector):\n",
    "    def to_rgb(x):\n",
    "        rgb = conv_2d(x, 3, 1, activation=None, name=f'to_rgb_{resolution}x{resolution}')\n",
    "        return rgb\n",
    "\n",
    "    resolution_to_channel_counts = {\n",
    "        4: 512, 8: 512, 16: 512, 32: 512, 64: 512, 128: 256, 256: 128, 512: 64, 1024: 32}\n",
    "    channel_count = resolution_to_channel_counts[resolution]\n",
    "    if resolution == 4:\n",
    "        block = ScaledDense(\n",
    "            channel_count*4*4,\n",
    "            name='latent_to_4x4')(latent_vector)\n",
    "        block = tf.keras.layers.Reshape((4, 4, channel_count))(block)\n",
    "        block = activate(block)\n",
    "        block = conv_2d(block, channel_count, name='conv_4x4')\n",
    "        rgb = to_rgb(block)\n",
    "        return rgb, block\n",
    "    else:\n",
    "        lower_res_rgb, lower_res_block = create_generator_body(resolution // 2, latent_vector)\n",
    "        name_base = f'conv_{resolution}x{resolution}'\n",
    "\n",
    "        block = lower_res_block\n",
    "        block = activate(UpsampleConv2d(channel_count, name=f'{name_base}_1')(block))\n",
    "        block = conv_2d(block, channel_count, name=f'{name_base}_2')\n",
    "        rgb = to_rgb(block)\n",
    "\n",
    "        lower_res_rgb = Upsample()(lower_res_rgb)\n",
    "        rgb = tf.keras.layers.Add()([lower_res_rgb, rgb])\n",
    "        return rgb, block\n",
    "\n",
    "\n",
    "def create_generator(output_resolution: int, latent_vector_size: int ) -> tf.keras.Model:\n",
    "    validate_resolution(output_resolution)\n",
    "\n",
    "    latent_vector = tf.keras.layers.Input((latent_vector_size,))\n",
    "    normalized_latent_vector = PixelNorm()(latent_vector)\n",
    "\n",
    "    model_rgb, _ = create_generator_body(output_resolution, normalized_latent_vector)\n",
    "    tensorflow_rgb = ImageConversion(ImageConversionMode.MODEL_TO_TENSORFLOW)(model_rgb)\n",
    "\n",
    "    # Cast the output to float32. Needed when using mixed_float16.\n",
    "    if tensorflow_rgb.dtype != tf.float32:\n",
    "        tensorflow_rgb = tf.keras.layers.Activation(None, dtype=tf.float32)(tensorflow_rgb)\n",
    "\n",
    "    return tf.keras.Model(inputs=latent_vector, outputs=tensorflow_rgb)\n",
    "\n",
    "\n",
    "def make_discriminator_body(tf_format_image, resolution):\n",
    "    resolution_to_feature_counts: Dict[int, Tuple[int, int]] = {\n",
    "        1024: (32, 64),\n",
    "        512: (64, 128),\n",
    "        256: (128, 256),\n",
    "        128: (256, 512),\n",
    "        64: (512, 512),\n",
    "        32: (512, 512),\n",
    "        16: (512, 512),\n",
    "        8: (512, 512),\n",
    "        4: (512, 512)}\n",
    "    feature_counts = resolution_to_feature_counts[resolution]\n",
    "\n",
    "    incoming_block = None\n",
    "    if tf_format_image.shape[1] > resolution:\n",
    "        incoming_block = make_discriminator_body(tf_format_image, resolution*2)\n",
    "    else:\n",
    "        model_format_image = ImageConversion(ImageConversionMode.TENSORFLOW_TO_MODEL)(tf_format_image)\n",
    "        incoming_block = conv_2d(\n",
    "            model_format_image,\n",
    "            feature_counts[0],\n",
    "            kernel_size=1,\n",
    "            name=f'from_rgb_{resolution}x{resolution}')\n",
    "\n",
    "    if resolution == 4:\n",
    "        block = incoming_block\n",
    "        block = MinibatchStddev()(block)\n",
    "        block = conv_2d(block, feature_counts[0], name=f'conv_4x4_1')\n",
    "        block = conv_2d(block, feature_counts[1], kernel_size=4, padding='valid', name=f'conv_4x4_2')\n",
    "        return block\n",
    "    else:\n",
    "        name_base = f'conv_{resolution}x{resolution}'\n",
    "        block = incoming_block\n",
    "        block = conv_2d(block, feature_counts[0], name=f'{name_base}_1')\n",
    "        block = conv_2d(block, feature_counts[1], strides=2, pre_blur=True, name=f'{name_base}_2')\n",
    "\n",
    "        shortcut = Downsample()(incoming_block)\n",
    "        if shortcut.shape[-1] != block.shape[-1]:\n",
    "            shortcut = conv_2d(\n",
    "                shortcut,\n",
    "                block.shape[-1],\n",
    "                kernel_size=1,\n",
    "                activation=None,\n",
    "                name=f'shortcut_{resolution}x{resolution}')\n",
    "        block = ScaledAdd()([block, shortcut])\n",
    "        return block\n",
    "\n",
    "\n",
    "def create_discriminator(input_resolution: int) -> tf.keras.Model:\n",
    "    validate_resolution(input_resolution)\n",
    "\n",
    "    image = tf.keras.layers.Input((input_resolution, input_resolution, 3))\n",
    "\n",
    "    x = make_discriminator_body(image, 4)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    classification = ScaledDense(\n",
    "            1,\n",
    "            name='to_classification')(x)\n",
    "    if classification.dtype != tf.float32:\n",
    "        classification = tf.keras.layers.Activation(None, dtype=tf.float32)(classification)\n",
    "\n",
    "    return tf.keras.Model(inputs=image, outputs=classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf7634-1eca-4cdb-8b9d-1362e2a8cad3",
   "metadata": {},
   "source": [
    "### Perception differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79ec4a-6447-4edf-bf59-7ec8b779ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(x):\n",
    "    return x / tf.sqrt(tf.reduce_sum(tf.square(x), axis=-1, keepdims=True) + 1e-7)\n",
    "\n",
    "\n",
    "def calculate_feature_pair_difference(a, b):\n",
    "    a = normalize_channels(a)\n",
    "    b = normalize_channels(b)\n",
    "\n",
    "    diffs = tf.linalg.norm(a - b, axis=-1)\n",
    "    return tf.reduce_mean(diffs, axis=[1, 2])\n",
    "\n",
    "def calculate_perceptual_difference(feature_pairs):\n",
    "    difference = None\n",
    "    for a, b in feature_pairs:\n",
    "        pair_diff = calculate_feature_pair_difference(a, b)\n",
    "        difference = pair_diff if difference is None else difference + pair_diff\n",
    "    difference /= len(feature_pairs)\n",
    "    return difference\n",
    "\n",
    "\n",
    "def create_feature_extractor():\n",
    "    def create_vgg_feature_extractor():\n",
    "        layer_names = [\n",
    "            # 'block1_conv2',\n",
    "            # 'block2_conv2',\n",
    "            # 'block3_conv3',\n",
    "            'block4_conv3',\n",
    "            'block5_conv3',\n",
    "            ]\n",
    "\n",
    "        vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False\n",
    "\n",
    "        outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "        return tf.keras.Model([vgg.input], outputs)\n",
    "\n",
    "    vgg_features = create_vgg_feature_extractor()\n",
    "    image = tf.keras.layers.Input((224, 224, 3))\n",
    "\n",
    "    def preprocess(image):\n",
    "        return tf.keras.applications.vgg16.preprocess_input(image * 255.)\n",
    "\n",
    "    preprocessed = tf.keras.layers.Lambda(preprocess)(image)\n",
    "    features = vgg_features(preprocessed)\n",
    "    return tf.keras.Model(image, features)\n",
    "\n",
    "\n",
    "def create_perceptual_difference_model():\n",
    "    feature_extractor = create_feature_extractor()\n",
    "\n",
    "    input_shape = feature_extractor.input_shape[1:]\n",
    "\n",
    "    image_a = tf.keras.layers.Input(input_shape)\n",
    "    image_b = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    features_a = feature_extractor(image_a)\n",
    "    features_b = feature_extractor(image_b)\n",
    "\n",
    "    if not isinstance(features_a, list):\n",
    "        features_a = [features_a]\n",
    "        features_b = [features_b]\n",
    "\n",
    "    diff = tf.keras.layers.Lambda(calculate_perceptual_difference)(list(zip(features_a, features_b)))\n",
    "\n",
    "    return tf.keras.Model([image_a, image_b], diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9fc748-5917-47a0-b93b-4e3f1afdb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checkpointer:\n",
    "    def __init__(self, file_format: str):\n",
    "        '''\n",
    "            Args:\n",
    "                file_format: format string with variable {checkpoint_i}.\n",
    "                    Ex. '/my/path/{checkpoint_i}.checkpoint'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.file_format = file_format\n",
    "\n",
    "    def path_for_checkpoint(self, checkpoint_i: Union[int, str]) -> str:\n",
    "        return self.file_format.format(checkpoint_i=checkpoint_i)\n",
    "\n",
    "    def list_checkpoints(self) -> List[int]:\n",
    "        file_names = tf.io.gfile.glob(self.path_for_checkpoint('*'))\n",
    "\n",
    "        def file_name_to_checkpoint_index(file_name: str) -> Optional[int]:\n",
    "            try:\n",
    "                return int(os.path.splitext(os.path.basename(file_name))[0])\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        checkpoint_is = list(\n",
    "            filter(\n",
    "                lambda i: i is not None,\n",
    "                map(file_name_to_checkpoint_index, file_names)))\n",
    "        checkpoint_is.sort()\n",
    "        return checkpoint_is\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_i: int, content: bytes) -> None:\n",
    "        bk_io.write_binary_file(self.path_for_checkpoint(checkpoint_i), content)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_i: int) -> bytes:\n",
    "        return bk_io.read_binary_file(self.path_for_checkpoint(checkpoint_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df10be-282d-4879-acca-376362d03fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def make_real_image_dataset(\n",
    "        batch_size: int,\n",
    "        file_pattern: str = 'gs://bk-ffhq/512x512/*.tfrecord',\n",
    "        randomly_flip: bool = True\n",
    "        ) -> tf.data.Dataset:\n",
    "    file_names = tf.io.gfile.glob(file_pattern)\n",
    "\n",
    "    def apply_flip(x):\n",
    "        if randomly_flip and tf.random.uniform((), minval=0., maxval=1., dtype=tf.float32) > 0.5:\n",
    "            x = x[:, ::-1, :]\n",
    "        return x\n",
    "\n",
    "    return tf.data.TFRecordDataset(file_names\n",
    "        ).map(decode_record_image\n",
    "        ).map(lambda image: tf.image.convert_image_dtype(image, tf.float32, saturate=True)\n",
    "        ).shuffle(1000\n",
    "        ).repeat(\n",
    "        ).map(apply_flip\n",
    "        ).batch(batch_size\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def create_visualizer_noise(\n",
    "        visualization_grid_size: Tuple[int, int],\n",
    "        latent_size: int) -> tf.Tensor:\n",
    "    if latent_size == 3:\n",
    "        lat_domain = tf.linspace(-math.pi / 2., math.pi / 2., visualization_grid_size[0])\n",
    "        lon_domain = tf.linspace(-math.pi, math.pi, visualization_grid_size[1])\n",
    "        lons, lats = tf.meshgrid(lon_domain, lat_domain)\n",
    "        lons, lats = tf.reshape(lons, [-1]), tf.reshape(lats, [-1])\n",
    "        xs = tf.cos(lats) * tf.cos(lons)\n",
    "        ys = tf.cos(lats) * tf.sin(lons)\n",
    "        zs = tf.sin(lats)\n",
    "        return tf.stack([xs, ys, zs], axis=-1)\n",
    "\n",
    "    return tf.random.normal((visualization_grid_size[0] * visualization_grid_size[1], latent_size))\n",
    "\n",
    "\n",
    "class TrainingOptions:\n",
    "    def __init__(\n",
    "            self,\n",
    "            visualization_grid_size: Tuple[int, int],\n",
    "            resolution: int,\n",
    "            replica_batch_size: int,\n",
    "            epoch_sample_count: int = 1024 * 16,\n",
    "            total_sample_count: int = 1024 * 800,\n",
    "            learning_rate: float = 0.002,\n",
    "            real_images_file_pattern: str = 'gs://bk-ffhq/1024x1024/*.tfrecord',\n",
    "            latent_size = 512,\n",
    "            randomly_flip_data: bool = True,\n",
    "            checkpoint_interval: int = 10,\n",
    "            visualizer_noise: Optional[tf.Tensor] = None,\n",
    "            visualization_smoothing_sample_count: int = 10000\n",
    "            ):\n",
    "        assert epoch_sample_count % replica_batch_size == 0\n",
    "        assert total_sample_count % epoch_sample_count == 0\n",
    "\n",
    "        self.visualization_grid_size = visualization_grid_size\n",
    "        self.resolution = resolution\n",
    "        self.replica_batch_size = replica_batch_size\n",
    "        self.epoch_sample_count = epoch_sample_count\n",
    "        self.total_sample_count = total_sample_count\n",
    "        self.learning_rate = learning_rate\n",
    "        self.real_images_file_pattern = real_images_file_pattern\n",
    "        self.latent_size = latent_size\n",
    "        self.randomly_flip_data = randomly_flip_data\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.visualizer_noise = default_to(\n",
    "            visualizer_noise,\n",
    "            create_visualizer_noise(visualization_grid_size, latent_size))\n",
    "        expected_noise_shape = (visualization_grid_size[0] * visualization_grid_size[1], latent_size)\n",
    "        assert self.visualizer_noise.shape == expected_noise_shape\n",
    "        self.visualization_smoothing_sample_count = visualization_smoothing_sample_count\n",
    "\n",
    "    @property\n",
    "    def epoch_count(self):\n",
    "        return self.total_sample_count // self.epoch_sample_count\n",
    "\n",
    "\n",
    "class TrainingState:\n",
    "    def __init__(\n",
    "            self,\n",
    "            options: TrainingOptions,\n",
    "            generator: Optional[tf.keras.Model] = None,\n",
    "            visualization_generator: Optional[tf.keras.Model] = None,\n",
    "            discriminator: Optional[tf.keras.Model] = None,\n",
    "            epoch_i: int = 0):\n",
    "        self.options = options\n",
    "        self.generator = generator\n",
    "        self.visualization_generator = visualization_generator\n",
    "        self.discriminator = discriminator\n",
    "        self.epoch_i = epoch_i\n",
    "\n",
    "    def training_is_done(self) -> bool:\n",
    "        return self.epoch_i * self.options.epoch_sample_count >= self.options.total_sample_count\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state['generator'] = serialize_model(self.generator)\n",
    "        state['visualization_generator'] = serialize_model(self.visualization_generator)\n",
    "        state['discriminator'] = serialize_model(self.discriminator)\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__ = state.copy()\n",
    "        self.generator = deserialize_model(\n",
    "            self.generator,\n",
    "            functools.partial(create_generator, self.options.resolution, self.options.latent_size))\n",
    "        self.visualization_generator = deserialize_model(\n",
    "            self.visualization_generator,\n",
    "            functools.partial(create_generator, self.options.resolution, self.options.latent_size))\n",
    "        self.discriminator = deserialize_model(\n",
    "            self.discriminator,\n",
    "            functools.partial(create_discriminator, self.options.resolution))\n",
    "\n",
    "\n",
    "class CheckpointStateCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state: TrainingState,\n",
    "            checkpointer: Checkpointer):\n",
    "        self.state = state\n",
    "        self.checkpointer = checkpointer\n",
    "        super().__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch_i: int, logs=None) -> None:\n",
    "        self.state.epoch_i = epoch_i + 1\n",
    "        if self.state.epoch_i % self.state.options.checkpoint_interval == 0:\n",
    "            self.checkpointer.save_checkpoint(self.state.epoch_i, serialize(self.state))\n",
    "\n",
    "\n",
    "def train(\n",
    "        strategy: tf.distribute.Strategy,\n",
    "        checkpointer: Checkpointer,\n",
    "        state: TrainingState,\n",
    "        on_visualization_callbacks: List[Callable[[int, tf.Tensor], None]] = list()\n",
    "        ) -> None:\n",
    "    options = state.options\n",
    "    def create_visualizer() -> GeneratorVisualizer:\n",
    "        replica_batch_size = min(options.replica_batch_size, 8)\n",
    "        visualization_image_count = options.visualization_grid_size[0] * options.visualization_grid_size[1]\n",
    "        assert visualization_image_count % strategy.num_replicas_in_sync == 0\n",
    "        max_replica_batch_size = visualization_image_count // strategy.num_replicas_in_sync\n",
    "        if (replica_batch_size > max_replica_batch_size or\n",
    "            visualization_image_count % (replica_batch_size * strategy.num_replicas_in_sync) != 0):\n",
    "            replica_batch_size = max_replica_batch_size\n",
    "\n",
    "        return GeneratorVisualizer(\n",
    "            strategy,\n",
    "            options.visualization_grid_size,\n",
    "            state.visualization_generator,\n",
    "            options.visualizer_noise,\n",
    "            update_interval=1,\n",
    "            on_image_callbacks=on_visualization_callbacks,\n",
    "            replica_batch_size=replica_batch_size)\n",
    "\n",
    "    checkpoint_callback = CheckpointStateCallback(state, checkpointer)\n",
    "\n",
    "    if state.generator is None:\n",
    "        global_batch_size = options.replica_batch_size * strategy.num_replicas_in_sync\n",
    "        with strategy.scope():\n",
    "            state.generator = create_generator(options.resolution, options.latent_size)\n",
    "            state.visualization_generator = create_generator(\n",
    "                options.resolution,\n",
    "                options.latent_size)\n",
    "            state.discriminator = create_discriminator(options.resolution)\n",
    "\n",
    "        @tf.function\n",
    "        def zero_vars(var):\n",
    "            for v in var:\n",
    "                v.assign(tf.zeros_like(v))\n",
    "        strategy.run(zero_vars, args=(state.visualization_generator.trainable_variables,))\n",
    "\n",
    "    global_batch_size = options.replica_batch_size * strategy.num_replicas_in_sync\n",
    "\n",
    "    visualization_weight_decay = (\n",
    "        0.5 ** (global_batch_size / options.visualization_smoothing_sample_count)\n",
    "        if options.visualization_smoothing_sample_count > 0 else\n",
    "        0.0)\n",
    "    update_visualization_generator_callback = UpdateVisualizationGeneratorCallback(\n",
    "        strategy,\n",
    "        state.generator,\n",
    "        state.visualization_generator,\n",
    "        visualization_weight_decay)\n",
    "\n",
    "    visualizer = create_visualizer()\n",
    "\n",
    "\n",
    "    image_dataset = strategy.experimental_distribute_dataset(\n",
    "        make_real_image_dataset(\n",
    "            global_batch_size,\n",
    "            file_pattern=options.real_images_file_pattern,\n",
    "            randomly_flip=options.randomly_flip_data))\n",
    "\n",
    "    state.epoch_i = training_loop(\n",
    "        strategy,\n",
    "        state.generator,\n",
    "        state.discriminator,\n",
    "        image_dataset,\n",
    "        state.epoch_i,\n",
    "        options.epoch_count,\n",
    "        options.replica_batch_size,\n",
    "        options.epoch_sample_count,\n",
    "        learning_rate=options.learning_rate,\n",
    "        callbacks=[update_visualization_generator_callback, visualizer, checkpoint_callback])\n",
    "    checkpointer.save_checkpoint(state.epoch_i, serialize(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8054e-8db5-410a-bd09-d2b9fb5f433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyStrategyScope:\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DummyStrategy:\n",
    "    def __init__(self):\n",
    "        self.num_replicas_in_sync = 1\n",
    "    def scope(self):\n",
    "        return DummyStrategyScope()\n",
    "    def experimental_distribute_dataset(self, dataset, *args):\n",
    "        return dataset\n",
    "    def run(self, f, args=()):\n",
    "        return f(*args)\n",
    "    def reduce(self, mode, value, axis=None):\n",
    "        return value\n",
    "    def gather(self, t, axis):\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811f8b5-0157-4881-93d1-5f492fd186de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97016d40-d36a-4a78-a5ff-a17ae922ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generator(checkpoint_folder_path: os.PathLike, checkpoint_i: Optional[int] = None):\n",
    "    checkpointer = Checkpointer(\n",
    "        os.path.join(checkpoint_folder_path, '{checkpoint_i}.checkpoint'))\n",
    "    checkpoint_i = default_to(checkpoint_i, max(checkpointer.list_checkpoints()))\n",
    "    training_state: TrainingState = deserialize(checkpointer.load_checkpoint(checkpoint_i))\n",
    "    return training_state.visualization_generator\n",
    "\n",
    "\n",
    "def calc_magnitude(v):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(v), axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "# Normalize batch of vectors.\n",
    "def normalize(v, magnitude=1.0):\n",
    "    return v * magnitude / tf.sqrt(tf.reduce_sum(tf.square(v), axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "# Spherical interpolation of a batch of vectors.\n",
    "def slerp(a, b, t):\n",
    "    norm_mag = lerp(calc_magnitude(a), calc_magnitude(b), t)\n",
    "    a = normalize(a)\n",
    "    b = normalize(b)\n",
    "    d = tf.reduce_sum(a * b, axis=-1, keepdims=True)\n",
    "    p = t * tf.math.acos(d)\n",
    "    c = normalize(b - d * a)\n",
    "    d = a * tf.math.cos(p) + c * tf.math.sin(p)\n",
    "    return normalize(d, norm_mag)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample_path_lengths_from(\n",
    "        generator: tf.keras.Model,\n",
    "        perceptual_differencer: tf.keras.Model,\n",
    "        origin_noise: tf.Tensor,\n",
    "        origin_images: tf.Tensor) -> tf.Tensor:\n",
    "    def resize_and_crop(images: tf.Tensor) -> tf.Tensor:\n",
    "        differ_image_size = perceptual_differencer.input_shape[0][1:3]\n",
    "        # Resize the images to be slightly larger than the differ's input size then crop the center\n",
    "        # to reduce the amount of background in the image and focus on the face.\n",
    "        images = tf.image.resize(\n",
    "            images,\n",
    "            (256 * differ_image_size[0] // 224, 256 * differ_image_size[1] // 224))\n",
    "        images = tf.image.resize_with_crop_or_pad(images, differ_image_size[0], differ_image_size[1])\n",
    "        return images\n",
    "\n",
    "    target_noises = slerp(origin_noise, tf.random.normal(origin_noise.shape), 0.1)\n",
    "    target_images = generator(target_noises)\n",
    "    origin_images, target_images = map(resize_and_crop, (origin_images, target_images))\n",
    "    return perceptual_differencer([origin_images, target_images])\n",
    "\n",
    "\n",
    "def calculate_perceptual_path_length(\n",
    "        generator: tf.keras.Model,\n",
    "        perceptual_differencer: tf.keras.Model,\n",
    "        noise: tf.Tensor,\n",
    "        images: tf.Tensor,\n",
    "        sample_count: int = 10):\n",
    "    diffs = [\n",
    "        sample_path_lengths_from(generator, perceptual_differencer, noise, images)\n",
    "        for _ in range(sample_count)]\n",
    "    avg_diff = tf.reduce_max(tf.square(tf.stack(diffs, axis=-1)), axis=-1)\n",
    "    return avg_diff\n",
    "\n",
    "\n",
    "def generate_random_images(\n",
    "        generator: tf.keras.Model,\n",
    "        args: argparse.Namespace):\n",
    "    perceptual_differencer = create_perceptual_difference_model()\n",
    "    batch_size = 4\n",
    "    noise_shape = (batch_size, generator.input_shape[-1])\n",
    "\n",
    "    prog_bar = tf.keras.utils.Progbar(args.sample_count)\n",
    "    generated_sample_count = 0\n",
    "    while generated_sample_count < args.sample_count:\n",
    "        noise = pixel_norm(tf.random.normal(noise_shape))\n",
    "        images = generator(noise)\n",
    "        ppls = (\n",
    "            calculate_perceptual_path_length(generator, perceptual_differencer, noise, images)\n",
    "            if args.threshold > 0 else\n",
    "            tf.zeros((batch_size,)))\n",
    "\n",
    "        for (image, score) in zip(images, ppls):\n",
    "            if args.threshold > 0 and score > args.threshold:\n",
    "                continue\n",
    "            file_name = os.path.join(args.out, f'{create_datetime_str()}_{score}.png')\n",
    "            save_image(file_name, image)\n",
    "            generated_sample_count += 1\n",
    "            prog_bar.add(1)\n",
    "            if generated_sample_count == args.sample_count:\n",
    "                break\n",
    "\n",
    "\n",
    "def generate_interpolation(\n",
    "        generator: tf.keras.Model,\n",
    "        start_noise: tf.Tensor,\n",
    "        end_noise: tf.Tensor) -> List[tf.Tensor]:\n",
    "    def generate_image(factor):\n",
    "        noise = slerp(start_noise, end_noise, factor)\n",
    "        return generator(noise)[0]\n",
    "    return list(map(generate_image, tf.linspace(0.0, 1.0, 100)))\n",
    "\n",
    "\n",
    "def generate_random_interpolations(\n",
    "        generator: tf.keras.Model,\n",
    "        args: argparse.Namespace):\n",
    "    def generate_random_interpolation():\n",
    "        noise_shape = (1, generator.input_shape[-1])\n",
    "        start_noise = pixel_norm(tf.random.normal(noise_shape))\n",
    "        end_noise = pixel_norm(tf.random.normal(noise_shape))\n",
    "        return generate_interpolation(generator, start_noise, end_noise)\n",
    "\n",
    "    prog_bar = tf.keras.utils.Progbar(args.sample_count)\n",
    "    for _ in range(args.sample_count):\n",
    "        interpolation_folder = os.path.join(args.out, create_datetime_str())\n",
    "        images = generate_random_interpolation()\n",
    "        for image_i, image in enumerate(images):\n",
    "            file_path = interpolation_folder / f'{image_i}.png'\n",
    "            save_image(file_path, image)\n",
    "        prog_bar.add(1)\n",
    "\n",
    "\n",
    "def generate_circular_interpolation(\n",
    "        generator: tf.keras.Model,\n",
    "        noise_origin: tf.Tensor,\n",
    "        noise_pass_through_point: tf.Tensor,\n",
    "        step_count: int = 500) -> List[tf.Tensor]:\n",
    "    assert noise_origin.shape[0] == 1\n",
    "    assert noise_pass_through_point.shape[0] == 1\n",
    "\n",
    "    dot_product = tf.reduce_sum(noise_origin * noise_pass_through_point, axis=-1)[0]\n",
    "    angular_distance = tf.math.acos(dot_product)\n",
    "    assert tf.abs(angular_distance) > 1e-7\n",
    "\n",
    "    full_revolution_factor = 2. * math.pi / angular_distance\n",
    "\n",
    "    images = []\n",
    "    for factor in tf.linspace(0.0, full_revolution_factor, step_count):\n",
    "        noise = slerp(noise_origin, noise_pass_through_point, factor)\n",
    "        images.append(generator(noise)[0])\n",
    "    return images\n",
    "\n",
    "\n",
    "def generate_random_circular_interpolations(\n",
    "        generator: tf.keras.Model,\n",
    "        args: argparse.Namespace):\n",
    "    def generate_random_circular_interpolation():\n",
    "        noise_shape = (1, generator.input_shape[-1])\n",
    "        noise_origin = normalize(tf.random.normal(noise_shape))\n",
    "        noise_pass_through_point = normalize(tf.random.normal(noise_shape))\n",
    "        return generate_circular_interpolation(generator, noise_origin, noise_pass_through_point)\n",
    "\n",
    "    prog_bar = tf.keras.utils.Progbar(args.sample_count)\n",
    "    for _ in range(args.sample_count):\n",
    "        interpolation_folder = os.path.join(args.out, create_datetime_str())\n",
    "        images = generate_random_circular_interpolation()\n",
    "        for image_i, image in enumerate(images):\n",
    "            file_path = interpolation_folder / f'{image_i}.png'\n",
    "            save_image(file_path, image)\n",
    "        prog_bar.add(1)\n",
    "\n",
    "\n",
    "# Callable function\n",
    "def generate_images():\n",
    "    limit_gpu_memory_usage(4*1024)\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Sample images from a trained generator',\n",
    "        fromfile_prefix_chars='@')\n",
    "\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    images_parser = subparsers.add_parser('images', help='Sample random images from the generator')\n",
    "    images_parser.add_argument(\n",
    "        '--threshold',\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help='Threshold for max perceptual path length filter. 0 for unlimited.')\n",
    "    images_parser.set_defaults(action=generate_random_images)\n",
    "\n",
    "    interpolation_parser = subparsers.add_parser(\n",
    "        'interpolations',\n",
    "        help='Sample interpolations from randomly chosen start and end points in the the latent ' +\n",
    "            'space')\n",
    "    interpolation_parser.set_defaults(action=generate_random_interpolations)\n",
    "\n",
    "    circular_parser = subparsers.add_parser(\n",
    "        'circular-interpolations',\n",
    "        help='Sample interpolations from a randomly chosen start point in the latent space that ' +\n",
    "            'travel in a random direction in a circle around the latent space back to the start ' +\n",
    "            'point.')\n",
    "    circular_parser.set_defaults(action=generate_random_circular_interpolations)\n",
    "\n",
    "    for subparser in [images_parser, interpolation_parser, circular_parser]:\n",
    "        subparser.add_argument('--sample_count', help='number of samples to generator', type=int, default=1)\n",
    "        subparser.add_argument('--checkpoint', help='checkpoint to load. Defaults highest checkpoint number.', type=int)\n",
    "        subparser.add_argument('checkpoint_folder', help='folder containing checkpoints')\n",
    "        subparser.add_argument('out', help='root output folder path')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    generator = load_generator(args.checkpoint_folder, args.checkpoint)\n",
    "    args.action(generator, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ea02c-46d2-4730-90e4-3152dbdfb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385596a-4b20-49b7-b6a7-fc92c2d5319e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b31bb44a-9a2d-4c93-9ecd-30c3d8e2ff40",
   "metadata": {},
   "source": [
    "### Create TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45470949-b6d6-4b06-9476-09acbec6739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RECORD_COUNT = 8\n",
    "\n",
    "def slice_dataset(dataset: tf.data.Dataset, start_index: int, increment: int):\n",
    "    def is_in_increment_group(index, value):\n",
    "        return index % increment == start_index\n",
    "\n",
    "    def unenumerate(index, value):\n",
    "        return value\n",
    "\n",
    "    return dataset.enumerate(\n",
    "        ).filter(is_in_increment_group\n",
    "        ).map(unenumerate)\n",
    "\n",
    "\n",
    "def resize_image(image: tf.Tensor, size: Tuple[int, int]) -> tf.Tensor:\n",
    "    original_dtype = image.dtype\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32, saturate=True)\n",
    "    # Computer Color is Broken - minutephysics\n",
    "    # https://youtu.be/LKnqECcg6Gw\n",
    "    image = tf.image.adjust_gamma(image, 2.2) # Gamma decode the image assuming a gamma of 2.2\n",
    "    image = tf.image.resize(image, size)\n",
    "    image = tf.image.adjust_gamma(image, 1./2.2) # Re-gamma-encode the image\n",
    "    image = tf.image.convert_image_dtype(image, original_dtype, saturate=True)\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_record(dataset: tf.data.Dataset, output_file_name: os.PathLike):\n",
    "    ensure_dir_for_file(output_file_name)\n",
    "    with tf.io.TFRecordWriter(str(output_file_name)) as file_writer:\n",
    "        for image in dataset:\n",
    "            file_writer.write(encode_record_image(image))\n",
    "\n",
    "\n",
    "def create_records_from_dataset(\n",
    "        dataset: tf.data.Dataset,\n",
    "        prepare,\n",
    "        output_resolution: Optional[int],\n",
    "        output_dir: os.PathLike,\n",
    "        record_count: int):\n",
    "    prog_bar = tf.keras.utils.Progbar(record_count)\n",
    "    for record_i in range(record_count):\n",
    "        record_dataset = slice_dataset(dataset, record_i, record_count)\n",
    "        if prepare is not None:\n",
    "            record_dataset = record_dataset.map(prepare)\n",
    "\n",
    "        if output_resolution is not None:\n",
    "            record_dataset = record_dataset.map(\n",
    "                lambda image: resize_image(image, (output_resolution, output_resolution)))\n",
    "\n",
    "        record_dataset = record_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        record_file_path = os.path.join(output_dir, f'{record_i}.tfrecord')\n",
    "        create_record(record_dataset, record_file_path)\n",
    "        prog_bar.add(1)\n",
    "\n",
    "\n",
    "def dataset_from_image_files(input_file_pattern: str):\n",
    "    def prepare(file_name: os.PathLike):\n",
    "        return tf.io.decode_image(tf.io.read_file(file_name))\n",
    "    input_file_names = tf.io.gfile.glob(input_file_pattern)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(input_file_names)\n",
    "    return dataset, prepare\n",
    "\n",
    "\n",
    "def dataset_from_tfrecords(record_file_pattern: str):\n",
    "    record_file_names = tf.io.gfile.glob(record_file_pattern)\n",
    "    dataset = tf.data.TFRecordDataset(record_file_names)\n",
    "    return dataset, decode_record_image\n",
    "\n",
    "\n",
    "def extract_record(record_name, output_dir, group_i: int = 0, group_count: int = 1):\n",
    "    dataset = tf.data.TFRecordDataset([record_name]\n",
    "        ).map(decode_record_image\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "    for i, image in enumerate(dataset):\n",
    "        file_name = os.path.join(output_dir, f'{i*group_count+group_i}.png')\n",
    "        save_image(file_name, image)\n",
    "\n",
    "\n",
    "def load_mnist_data() -> tf.data.Dataset:\n",
    "    def prepare(sample):\n",
    "        image = sample['image']\n",
    "        image = tf.broadcast_to(image, image.shape[:2] + (3,))\n",
    "        return image\n",
    "    return tfds.load('mnist', split='train').map(prepare)\n",
    "\n",
    "\n",
    "def create_records_from_args(args: argparse.Namespace):\n",
    "    def gather_args():\n",
    "        if args.source.lower() == 'mnist':\n",
    "            print('Creating dataset from MNIST')\n",
    "            return (\n",
    "                load_mnist_data(),\n",
    "                None,\n",
    "                default_to(args.resolution, 32),\n",
    "                default_to(args.record_count, DEFAULT_RECORD_COUNT))\n",
    "\n",
    "        if args.source.lower().endswith('tfrecord'):\n",
    "            print(f'Creating dataset from records: {args.source}')\n",
    "            dataset, prepare = dataset_from_tfrecords(args.source)\n",
    "            return (\n",
    "                dataset,\n",
    "                prepare,\n",
    "                args.resolution,\n",
    "                default_to(args.record_count, len(tf.io.gfile.glob(args.source))))\n",
    "\n",
    "        print(f'Creating dataset from images: {args.source}')\n",
    "        dataset, prepare = dataset_from_image_files(args.source)\n",
    "        return (\n",
    "            dataset,\n",
    "            prepare,\n",
    "            args.resolution,\n",
    "            default_to(args.record_count, DEFAULT_RECORD_COUNT))\n",
    "\n",
    "    source_dataset, prepare, resolution, record_count = gather_args()\n",
    "    create_records_from_dataset(\n",
    "        source_dataset,\n",
    "        prepare,\n",
    "        resolution,\n",
    "        args.output_folder_name,\n",
    "        record_count)\n",
    "\n",
    "\n",
    "def extract_records(args: argparse.Namespace):\n",
    "    print(f'Extracting records from {args.source}')\n",
    "    record_paths = tf.io.gfile.glob(args.source)\n",
    "    record_count = len(record_paths)\n",
    "\n",
    "    prog_bar = tf.keras.utils.Progbar(record_count)\n",
    "    for record_i, record_path in enumerate(record_paths):\n",
    "        extract_record(\n",
    "            record_path,\n",
    "            args.output_folder_name,\n",
    "            group_i=record_i,\n",
    "            group_count=record_count)\n",
    "        prog_bar.add(1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    limit_gpu_memory_usage()\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Create, resize, and extract tfrecords',\n",
    "        fromfile_prefix_chars='@')\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    create_parser = subparsers.add_parser(\n",
    "        'create',\n",
    "        help='create a set of tf records')\n",
    "    create_parser.set_defaults(action=create_records_from_args)\n",
    "\n",
    "    create_parser.add_argument(\n",
    "        '--resolution',\n",
    "        help='resolution to resize the images to. By default the images will remain their ' +\n",
    "            'pre-existing size with the exception of \"mnist\", which will be resized to 32 by '\n",
    "            'default.',\n",
    "        type=int)\n",
    "\n",
    "    create_parser.add_argument(\n",
    "        '--record-count',\n",
    "        help='number of tf records to split the data into. If the source is a set of tfrecords, ' +\n",
    "            'this will default to the number of records in the source. Otherwise, it will ' +\n",
    "            f'default to {DEFAULT_RECORD_COUNT}.',\n",
    "        type=int)\n",
    "\n",
    "    create_parser.add_argument(\n",
    "        'source',\n",
    "        help='a file glob pattern for images or records (*.tfrecord) to pack into the ' +\n",
    "            'destination records. You can pass the special value of \"mnist\" to download ' +\n",
    "            'the mnist dataset and pack it into the records.')\n",
    "\n",
    "    create_parser.add_argument(\n",
    "        'output_folder_name',\n",
    "        help='output directory to write the tf records')\n",
    "\n",
    "    extract_parser = subparsers.add_parser(\n",
    "        'extract',\n",
    "        help='extract the images from a set of tf records',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    extract_parser.set_defaults(action=extract_records)\n",
    "\n",
    "    extract_parser.add_argument(\n",
    "        'source',\n",
    "        help='a file glob pattern for the tf records to be extracted')\n",
    "\n",
    "    extract_parser.add_argument(\n",
    "        'output_folder_name',\n",
    "        help='output directory to write the images')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.action(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd4797-da01-4df1-a179-130daac7f963",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a390de6-4351-47db-92c6-a75759634d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_visualization(\n",
    "        visualization_folder_path: os.PathLike,\n",
    "        should_spherically_project: bool,\n",
    "        epoch_i: int,\n",
    "        images: tf.Tensor,\n",
    "        visualization_callback: Optional[Callable[[tf.Tensor], None]] = None):\n",
    "    image_file_name = f'{epoch_i+1}.png'\n",
    "\n",
    "    unprojected_image = images_to_gridded_image(images)\n",
    "    unprojected_folder_path = (\n",
    "        os.path.join(visualization_folder_path, 'unprojected')\n",
    "        if should_spherically_project else\n",
    "        visualization_folder_path)\n",
    "    save_image(os.path.join(unprojected_folder_path, image_file_name), unprojected_image)\n",
    "\n",
    "    if should_spherically_project:\n",
    "        projected_image = spherically_project_images_to_grid(images)\n",
    "        save_image(os.path.join(visualization_folder_path, 'projected', image_file_name), projected_image)\n",
    "\n",
    "    if visualization_callback:\n",
    "        visualization_callback(projected_image if should_spherically_project else unprojected_image)\n",
    "\n",
    "\n",
    "def create_visualization_callback(\n",
    "        root_output_path: os.PathLike,\n",
    "        latent_size: int,\n",
    "        visualization_callback: Optional[Callable[[tf.Tensor], None]]):\n",
    "    visualization_path = os.path.join(root_output_path, 'visualizations')\n",
    "    should_spherically_project = latent_size == 3\n",
    "    return functools.partial(\n",
    "        save_visualization,\n",
    "        visualization_path,\n",
    "        should_spherically_project,\n",
    "        visualization_callback=visualization_callback)\n",
    "\n",
    "\n",
    "def create_checkpointer(output_root: os.PathLike):\n",
    "    checkpointer_path = os.path.join(output_root, 'checkpoints', '{checkpoint_i}.checkpoint')\n",
    "    return Checkpointer(checkpointer_path)\n",
    "\n",
    "\n",
    "def init_training(\n",
    "        args: argparse.Namespace,\n",
    "        visualization_callback: Optional[Callable[[tf.Tensor], None]],\n",
    "        strategy: Optional[tf.distribute.Strategy]):\n",
    "    output_root = args.out\n",
    "    if args.create_unique_id:\n",
    "        output_root = os.path.join(output_root, create_datetime_str())\n",
    "\n",
    "    strategy = default_to(strategy, DummyStrategy())\n",
    "\n",
    "    checkpointer = create_checkpointer(output_root)\n",
    "    training_options = TrainingOptions(\n",
    "        tuple(args.visualization_grid_size),\n",
    "        args.resolution,\n",
    "        args.replica_batch_size,\n",
    "        epoch_sample_count=args.epoch_sample_count,\n",
    "        total_sample_count=args.total_sample_count,\n",
    "        real_images_file_pattern=args.dataset_file_pattern,\n",
    "        latent_size=args.latent_size,\n",
    "        checkpoint_interval=args.checkpoint_interval,\n",
    "        visualization_smoothing_sample_count=args.visualization_smoothing_sample_count,\n",
    "        randomly_flip_data=(not args.disable_horizontal_flip_data_augmentation))\n",
    "    training_state = TrainingState(training_options)\n",
    "\n",
    "    visualization_callback = create_visualization_callback(\n",
    "        output_root,\n",
    "        training_state.options.latent_size,\n",
    "        visualization_callback)\n",
    "\n",
    "    train(\n",
    "        strategy,\n",
    "        checkpointer,\n",
    "        training_state,\n",
    "        on_visualization_callbacks=[visualization_callback])\n",
    "\n",
    "def resume_training(\n",
    "        args: argparse.Namespace,\n",
    "        visualization_callback: Optional[Callable[[tf.Tensor], None]],\n",
    "        strategy: Optional[tf.distribute.Strategy]):\n",
    "    strategy = default_to(strategy, DummyStrategy())\n",
    "    checkpointer = create_checkpointer(args.out)\n",
    "    checkpoint_i = default_to(args.checkpoint, max(checkpointer.list_checkpoints()))\n",
    "\n",
    "    print(f'Resuming training from checkpoint {checkpoint_i}')\n",
    "\n",
    "    with strategy.scope():\n",
    "        training_state = deserialize(checkpointer.load_checkpoint(checkpoint_i))\n",
    "\n",
    "    visualization_callback = create_visualization_callback(\n",
    "        args.out,\n",
    "        training_state.options.latent_size,\n",
    "        visualization_callback)\n",
    "\n",
    "    train(\n",
    "        strategy,\n",
    "        checkpointer,\n",
    "        training_state,\n",
    "        on_visualization_callbacks=[visualization_callback])\n",
    "\n",
    "\n",
    "def main(\n",
    "        raw_arguments: Optional[List[str]] = None,\n",
    "        visualization_callback: Optional[Callable[[tf.Tensor], None]] = None,\n",
    "        strategy: Optional[tf.distribute.Strategy] = None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Train a GAN',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "        fromfile_prefix_chars='@')\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    init_parser = subparsers.add_parser(\n",
    "        'init',\n",
    "        help='initialize and begin training',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    init_parser.set_defaults(func=init_training)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--create-unique-id',\n",
    "        action='store_true',\n",
    "        help='if true, a unique id will be created from the date/time and appended to the output path')\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--visualization-grid-size',\n",
    "        type=int,\n",
    "        nargs=2,\n",
    "        help='height and width of the visualization grid. Ex: ... --visualization_grid_size 30 60 ...',\n",
    "        default=[4, 8])\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--resolution',\n",
    "        type=int,\n",
    "        help='resolution of generated images. Must match the resolution of the dataset',\n",
    "        default=512)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--replica-batch-size',\n",
    "        type=int,\n",
    "        help='size of batch per replica',\n",
    "        default=8)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--epoch-sample-count',\n",
    "        type=int,\n",
    "        help='number of samples per epoch. Must divide evenly into total-sample-count',\n",
    "        default=16*1024)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--total-sample-count',\n",
    "        type=int,\n",
    "        help='number of total samples to train on. Must be divisible by epoch-sample-count',\n",
    "        default=25*1024*1024)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--latent-size',\n",
    "        type=int,\n",
    "        help='size of the generator\\'s latent vector',\n",
    "        default=512)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--checkpoint-interval',\n",
    "        type=int,\n",
    "        help='interval of epochs to save a checkpoint',\n",
    "        default=10)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--visualization-smoothing-sample-count',\n",
    "        type=float,\n",
    "        help='the factor by which to decay the visualization weights. If 0, no smoothing will be applied.',\n",
    "        default=10000)\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        '--disable-horizontal-flip-data-augmentation',\n",
    "        action='store_true',\n",
    "        help='including this option will disable horizontal flip data augmentation. Use when ' +\n",
    "            'horizontally flipping an image changes its semantics, ex., mnist digits.')\n",
    "\n",
    "    init_parser.add_argument(\n",
    "        'dataset_file_pattern',\n",
    "        help='GLOB pattern for the dataset files. Ex: \\'D:/datasets/ffhq/1024x1024/*.tfrecord\\'')\n",
    "\n",
    "    resume_parser = subparsers.add_parser('resume', help='resume training from a checkpoint')\n",
    "    resume_parser.set_defaults(func=resume_training)\n",
    "\n",
    "    for subparser in [init_parser, resume_parser]:\n",
    "        subparser.add_argument(\n",
    "            '--gpu-mem-limit',\n",
    "            help='maximum amount of memory to consume on the gpu in GB. 0 for unlimited',\n",
    "            type=int,\n",
    "            default=0)\n",
    "        subparser.add_argument('out', help='root output folder')\n",
    "\n",
    "    resume_parser.add_argument(\n",
    "        '--checkpoint',\n",
    "        help='checkpoint epoch to resume from. Defaults to the largest checkpointed epoch',\n",
    "        type=int)\n",
    "\n",
    "    args = parser.parse_args(args=raw_arguments)\n",
    "\n",
    "    if args.gpu_mem_limit != 0:\n",
    "        limit_gpu_memory_usage(args.gpu_mem_limit * 1024)\n",
    "\n",
    "    args.func(args, visualization_callback, strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f83f26-7b28-4b8a-9cfd-1239fc2b64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059703de-7e5c-4b80-8cc1-fbbaafc70a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "        strategy: tf.distribute.Strategy,\n",
    "        generator: tf.keras.Model,\n",
    "        discriminator: tf.keras.Model,\n",
    "        real_image_dataset: tf.distribute.DistributedDataset,\n",
    "        epoch_i: int,\n",
    "        end_epoch_i: int,\n",
    "        replica_batch_size: int,\n",
    "        epoch_sample_count: int,\n",
    "        learning_rate: float = 0.002,\n",
    "        beta_1: float = 0.0,\n",
    "        beta_2: float = 0.99,\n",
    "        d_regularization_interval: int = 16,\n",
    "        callbacks: List[tf.keras.callbacks.Callback] = []\n",
    "        ) -> int:\n",
    "    global_batch_size = replica_batch_size * strategy.num_replicas_in_sync\n",
    "    assert epoch_sample_count % global_batch_size == 0\n",
    "    epoch_batch_count = epoch_sample_count // global_batch_size\n",
    "\n",
    "    noise_size = generator.inputs[0].shape[-1]\n",
    "\n",
    "    d_stat_names = ['d_loss', 'd_real', 'd_fake']\n",
    "    g_stat_names = ['g_loss']\n",
    "\n",
    "    progbar_callback = tf.keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "    progbar_callback.target = epoch_batch_count\n",
    "    callback_list = tf.keras.callbacks.CallbackList(\n",
    "        callbacks=[progbar_callback] + callbacks,\n",
    "        model=generator)\n",
    "\n",
    "    with strategy.scope():\n",
    "        generator.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2)\n",
    "\n",
    "        lazy_ratio = d_regularization_interval / (d_regularization_interval + 1)\n",
    "        discriminator.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate*lazy_ratio,\n",
    "            beta_1=beta_1**lazy_ratio,\n",
    "            beta_2=beta_2**lazy_ratio)\n",
    "\n",
    "    def reduce_across_batch(x: tf.Tensor) -> tf.Tensor:\n",
    "        return tf.reduce_sum(x) / global_batch_size\n",
    "\n",
    "    @tf.function\n",
    "    def take_g_step() -> Dict[str, tf.Tensor]:\n",
    "        noise = tf.random.normal(shape=(replica_batch_size, noise_size))\n",
    "        fake_images = generator(noise, training=True)\n",
    "        fake_classifications = discriminator(fake_images, training=False)\n",
    "        loss = reduce_across_batch(tf.nn.softplus(-fake_classifications))\n",
    "\n",
    "        grads = tf.gradients(loss, generator.trainable_variables)\n",
    "\n",
    "        assert len(generator.trainable_variables) == len(grads)\n",
    "        generator.optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "\n",
    "        return {'g_loss': loss}\n",
    "\n",
    "    @tf.function\n",
    "    def take_d_classification_step(real_images) -> Dict[str, tf.Tensor]:\n",
    "        noise = tf.random.normal(shape=(replica_batch_size, noise_size))\n",
    "        fake_images = generator(noise, training=False)\n",
    "        real_classifications = discriminator(real_images, training=True)\n",
    "        fake_classifications = discriminator(fake_images, training=True)\n",
    "\n",
    "        real_loss = reduce_across_batch(tf.nn.softplus(-real_classifications))\n",
    "        fake_loss = reduce_across_batch(tf.nn.softplus(fake_classifications))\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_grads = tf.gradients(d_loss, discriminator.trainable_variables)\n",
    "        assert len(d_grads) == len(discriminator.trainable_variables)\n",
    "        discriminator.optimizer.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "\n",
    "        stats = [d_loss, real_loss, fake_loss]\n",
    "        assert len(stats) == len(d_stat_names)\n",
    "        stat_dict = dict(zip(d_stat_names, stats))\n",
    "\n",
    "        return stat_dict\n",
    "\n",
    "    @tf.function\n",
    "    def take_d_reg_step(real_images) -> Dict[str, tf.Tensor]:\n",
    "        real_classifications = discriminator(real_images, training=True)\n",
    "        real_grads = tf.gradients(tf.reduce_sum(real_classifications), real_images)\n",
    "        gradient_loss = reduce_across_batch(tf.reduce_sum(tf.square(real_grads), axis=[1, 2, 3]))\n",
    "        gradient_penalty_strength = 10. * 0.5 * d_regularization_interval\n",
    "        gradient_penalty = gradient_loss * gradient_penalty_strength\n",
    "        reg_grads = tf.gradients(gradient_penalty, discriminator.trainable_variables)\n",
    "        assert len(reg_grads) == len(discriminator.trainable_variables)\n",
    "\n",
    "        # The final bias addition has a second derivative of 0 which tf.gradients reports as\n",
    "        # None. To prevent apply_gradients() from warning about this, we just insert a tensor\n",
    "        # full of zeros.\n",
    "        assert reg_grads[-1] is None\n",
    "        reg_grads[-1] = tf.zeros_like(discriminator.trainable_variables[-1])\n",
    "        discriminator.optimizer.apply_gradients(zip(reg_grads, discriminator.trainable_variables))\n",
    "\n",
    "        return {'d_grad_reg': gradient_penalty}\n",
    "\n",
    "    def tensor_dict_to_numpy(tensor_dict: Dict[str, tf.Tensor]) -> Dict[str, float]:\n",
    "        return {key: strategy.reduce('sum', value, axis=None).numpy() for key, value in tensor_dict.items()}\n",
    "\n",
    "    real_image_iter = iter(real_image_dataset)\n",
    "    callback_list.on_train_begin()\n",
    "    while epoch_i < end_epoch_i:\n",
    "        callback_list.on_epoch_begin(epoch_i)\n",
    "        all_stat_names = g_stat_names + d_stat_names + ['d_grad_reg', 'epoch']\n",
    "        epoch_stats = {key: 0. for key in all_stat_names}\n",
    "        epoch_stat_counts = epoch_stats.copy()\n",
    "        for batch_i in range(0, epoch_batch_count):\n",
    "            callback_list.on_train_batch_begin(batch_i)\n",
    "\n",
    "            batch_stats = {}\n",
    "            d_stats = strategy.run(take_d_classification_step, args=(next(real_image_iter),))\n",
    "            batch_stats.update(tensor_dict_to_numpy(d_stats))\n",
    "\n",
    "            global_batch_i = epoch_i * epoch_batch_count + batch_i\n",
    "            if global_batch_i % d_regularization_interval == 0:\n",
    "                d_reg_stats = strategy.run(take_d_reg_step, args=(next(real_image_iter),))\n",
    "                batch_stats.update(tensor_dict_to_numpy(d_reg_stats))\n",
    "\n",
    "            g_stats = strategy.run(take_g_step)\n",
    "            batch_stats.update(tensor_dict_to_numpy(g_stats))\n",
    "\n",
    "            batch_stats['epoch'] = epoch_i\n",
    "\n",
    "            for name, stat in batch_stats.items():\n",
    "                epoch_stats[name] = ((epoch_stats[name] * epoch_stat_counts[name] + stat) /\n",
    "                    (epoch_stat_counts[name] + 1))\n",
    "                epoch_stat_counts[name] += 1\n",
    "\n",
    "            callback_list.on_train_batch_end(batch_i, logs=batch_stats)\n",
    "\n",
    "        callback_list.on_epoch_end(epoch_i, logs=epoch_stats)\n",
    "        try:\n",
    "            mem_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "            print(f'Memory usage: {mem_info}')\n",
    "        except:\n",
    "            pass\n",
    "        epoch_i += 1\n",
    "    callback_list.on_train_end()\n",
    "    return epoch_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2473e2-4ff7-46f2-bd39-0a5982eb6721",
   "metadata": {},
   "source": [
    "# Plotting and Image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536964f8-f020-47c1-b7ad-c4d9cf5d9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_gridded_image(images: tf.Tensor) -> tf.Tensor:\n",
    "    '''\n",
    "        images should have shape [grid_height, grid_width, image_height, image_width, channel_count]\n",
    "    '''\n",
    "    grid_height, grid_width, image_height, image_width, channel_count = images.shape\n",
    "    grid = tf.transpose(images, perm=[0, 2, 1, 3, 4])\n",
    "    grid = tf.reshape(grid, (grid_height * image_height, grid_width * image_width, channel_count))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def spherically_project_images_to_grid(\n",
    "        images: tf.Tensor,\n",
    "        background_color: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
    "    '''\n",
    "        images should have shape [lat, lon, height, width, channels]\n",
    "    '''\n",
    "    grid_height, grid_width, _, _, _ = images.shape\n",
    "\n",
    "    background_color = default_to(background_color, tf.constant([128, 128, 128], dtype=tf.uint8))\n",
    "\n",
    "    assert background_color.dtype == images.dtype\n",
    "    projected = tf.Variable(tf.broadcast_to(background_color, images.shape))\n",
    "    cur_dist = tf.Variable(tf.fill((grid_height, grid_width), math.inf))\n",
    "\n",
    "    for cell_y, unproj_cell_x in [(y, x) for y in range(grid_height) for x in range(grid_width)]:\n",
    "        x_range = math.sin(cell_y / (grid_height - 1) * math.pi)\n",
    "        proj_cell_x = unproj_cell_x * x_range + grid_width * 0.5 * (1 - x_range)\n",
    "        int_proj_cell_x = round(proj_cell_x)\n",
    "        cell_center_dist = tf.abs(int_proj_cell_x - proj_cell_x)\n",
    "        if cell_center_dist < cur_dist[cell_y, int_proj_cell_x]:\n",
    "            cur_dist[cell_y, int_proj_cell_x].assign(cell_center_dist)\n",
    "            projected[cell_y, int_proj_cell_x].assign(images[cell_y, unproj_cell_x])\n",
    "\n",
    "    return images_to_gridded_image(projected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d550d9-9b17-40d9-86ae-10199a8a587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_models(create_model: Callable[[int], tf.keras.Model], model_name: str):\n",
    "    for resolution in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        print(resolution)\n",
    "        model = create_model(resolution)\n",
    "        file_path = os.path.join('output', 'model_plots', f'{model_name}_{resolution}x{resolution}.png')\n",
    "        ensure_dir_for_file(file_path)\n",
    "        tf.keras.utils.plot_model(model, file_path, show_shapes=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b935e6-da2d-4751-b106-116eef39857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model build and plot. \n",
    "limit_gpu_memory_usage(1024)\n",
    "\n",
    "plot_models(lambda resolution: create_generator(resolution, 512), 'generator')\n",
    "plot_models(create_discriminator, 'discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e2373-e7bf-4e13-baf2-ae816e8e9450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd3c54-2596-492e-9aa0-fcd328a2be24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8e868-1fa5-444c-a7bd-ffee13eeeedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
